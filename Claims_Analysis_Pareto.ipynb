{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3106b770",
   "metadata": {},
   "source": [
    "# Claims Performance Analysis & Pareto by Warehouse\n",
    "\n",
    "This notebook ingests an exported **dealer claims** spreadsheet, computes cycle times, and produces:\n",
    "1. **Bucket performance table** — count, mean, median, min, max cycle time (days) by *Bucket Name*.\n",
    "2. **Pareto of Labels (overall and by warehouse)** — count and cumulative percent, with a Pareto chart for each warehouse.\n",
    "3. **Power BI–ready outputs** — an Excel file with clean tables for direct import.\n",
    "\n",
    "> **Cycle time** = `Completed Date − Created Date` (in days).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b03039e",
   "metadata": {},
   "source": [
    "## Requirements & Assumptions\n",
    "- Input file is an Excel workbook (e.g., `Claims.xlsx`) with a sheet named **`Tasks`** exported from your tracking system.\n",
    "- The `Tasks` sheet includes at least these columns:\n",
    "  - `Bucket Name` (warehouse/location buckets: e.g., OKC, Atlanta, Orlando, Ontario, El Paso, Flowood, Phoenix, Charlotte — plus any admin/problem buckets)\n",
    "  - `Labels` (category for the event/claim; may be a single label or multiple labels separated by `;`)\n",
    "  - `Created Date`\n",
    "  - `Completed Date`\n",
    "- **Warehouse list** used for *by-warehouse* Pareto is configurable (defaults to your eight sites).\n",
    "\n",
    "> If some rows do not have a completed date, their cycle time will be `NaN` and excluded from cycle-time aggregations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f13f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Standard imports\n",
    "from pathlib import Path\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Display options (purely for notebook readability)\n",
    "pd.set_option(\"display.max_rows\", 50)\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "pd.set_option(\"display.width\", 120)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d7bcc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# ----------------------------\n",
    "# Parameters (EDIT THESE)\n",
    "# ----------------------------\n",
    "# Path to the input claims workbook. By default, looks for 'Claims.xlsx' in the same folder as this notebook.\n",
    "# If not found, it will try a fallback (used in ChatGPT sandbox), then raise an error.\n",
    "INPUT_XLSX = Path(\"Claims.xlsx\")\n",
    "FALLBACK = Path(\"/mnt/data/Claims.xlsx\")  # used only if running in ChatGPT sandbox\n",
    "\n",
    "# Output Excel for Power BI\n",
    "OUTPUT_XLSX = Path(\"Claims_Performance_Summary.xlsx\")\n",
    "\n",
    "# Warehouse names to include in \"by warehouse\" Pareto (case-sensitive match after stripping)\n",
    "WAREHOUSES = [\n",
    "    \"OKC\", \"Atlanta\", \"Orlando\", \"Ontario\", \"El Paso\", \"Flowood\", \"Phoenix\", \"Charlotte\"\n",
    "]\n",
    "\n",
    "# Maximum labels to show in each Pareto chart for readability (set None to show all)\n",
    "TOP_N_LABELS = 15\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21498351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# ----------------------------\n",
    "# Load data\n",
    "# ----------------------------\n",
    "def _resolve_input_path() -> Path:\n",
    "    if INPUT_XLSX.exists():\n",
    "        return INPUT_XLSX\n",
    "    if FALLBACK.exists():\n",
    "        return FALLBACK\n",
    "    raise FileNotFoundError(f\"Could not find input Excel at {INPUT_XLSX} or fallback {FALLBACK}\")\n",
    "\n",
    "in_path = _resolve_input_path()\n",
    "\n",
    "# Load the 'Tasks' sheet\n",
    "xls = pd.ExcelFile(in_path)\n",
    "if \"Tasks\" not in xls.sheet_names:\n",
    "    raise ValueError(f\"'Tasks' sheet not found. Available sheets: {xls.sheet_names}\")\n",
    "\n",
    "df = pd.read_excel(in_path, sheet_name=\"Tasks\")\n",
    "\n",
    "# Normalize key columns\n",
    "for col in [\"Bucket Name\", \"Labels\"]:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].astype(str).str.strip()\n",
    "    else:\n",
    "        raise ValueError(f\"Expected column '{col}' not found in Tasks sheet.\")\n",
    "\n",
    "# Date parsing\n",
    "for col in [\"Created Date\", \"Completed Date\"]:\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_datetime(df[col], errors=\"coerce\")\n",
    "    else:\n",
    "        raise ValueError(f\"Expected column '{col}' not found in Tasks sheet.\")\n",
    "\n",
    "# Compute cycle time (days)\n",
    "df[\"Cycle Time (Days)\"] = (df[\"Completed Date\"] - df[\"Created Date\"]).dt.days\n",
    "\n",
    "# Optional: treat negative durations as NaN if present (uncomment to enforce)\n",
    "# df.loc[df[\"Cycle Time (Days)\"] < 0, \"Cycle Time (Days)\"] = np.nan\n",
    "\n",
    "print(f\"Loaded {len(df):,} rows from {in_path.name}\")\n",
    "df.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415cbd81",
   "metadata": {},
   "source": [
    "## Bucket Performance (Cycle Times by *Bucket Name*)\n",
    "\n",
    "The table below aggregates cycle time statistics per **Bucket Name**:\n",
    "- **count** — number of rows with a valid cycle time\n",
    "- **mean**/**median** — average and median days to completion\n",
    "- **min**/**max** — shortest and longest observed cycle times\n",
    "\n",
    "This is exported to the `Bucket_Performance` sheet in the Power BI output workbook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144230eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# ----------------------------\n",
    "# Bucket performance stats\n",
    "# ----------------------------\n",
    "bucket_perf = (\n",
    "    df.groupby(\"Bucket Name\")[\"Cycle Time (Days)\"]\n",
    "      .agg([\"count\", \"mean\", \"median\", \"min\", \"max\"])\n",
    "      .reset_index()\n",
    "      .sort_values([\"Bucket Name\"])\n",
    ")\n",
    "\n",
    "# For readability, round floating-point columns (mean/median) to 2 decimals\n",
    "for c in [\"mean\", \"median\"]:\n",
    "    bucket_perf[c] = bucket_perf[c].round(2)\n",
    "\n",
    "bucket_perf.rename(columns={\n",
    "    \"count\": \"Count\",\n",
    "    \"mean\": \"Mean (days)\",\n",
    "    \"median\": \"Median (days)\",\n",
    "    \"min\": \"Min (days)\",\n",
    "    \"max\": \"Max (days)\"\n",
    "}, inplace=True)\n",
    "\n",
    "bucket_perf.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0094e56",
   "metadata": {},
   "source": [
    "## Pareto of Labels — Overall\n",
    "\n",
    "Counts are computed from the `Labels` column. If multiple labels exist in a single task row separated by `;`, they are split and each counted once.\n",
    "We then calculate **cumulative percent** in descending order of frequency.\n",
    "\n",
    "A classic Pareto interpretation: the first ~20% of categories often account for ~80% of occurrences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6656a59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# ----------------------------\n",
    "# Pareto - overall Labels\n",
    "# ----------------------------\n",
    "labels_df = df[[\"Bucket Name\", \"Labels\"]].copy()\n",
    "\n",
    "# Split labels on ';' and explode; strip whitespace and drop empties\n",
    "labels_exploded = (\n",
    "    labels_df.assign(Labels=labels_df[\"Labels\"].str.split(\";\"))\n",
    "             .explode(\"Labels\")\n",
    ")\n",
    "labels_exploded[\"Labels\"] = labels_exploded[\"Labels\"].astype(str).str.strip()\n",
    "labels_exploded = labels_exploded[labels_exploded[\"Labels\"].ne(\"\")]\n",
    "\n",
    "# Aggregation overall\n",
    "labels_overall = (\n",
    "    labels_exploded[\"Labels\"].value_counts()\n",
    "    .rename_axis(\"Label\").reset_index(name=\"Count\")\n",
    ")\n",
    "\n",
    "total_count = labels_overall[\"Count\"].sum()\n",
    "labels_overall[\"Cumulative %\"] = labels_overall[\"Count\"].cumsum() / total_count * 100\n",
    "labels_overall[\"Cumulative %\"] = labels_overall[\"Cumulative %\"].round(2)\n",
    "\n",
    "labels_overall.head(15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2840bc",
   "metadata": {},
   "source": [
    "## Pareto of Labels — By Warehouse\n",
    "\n",
    "For the *by-warehouse* Pareto, we filter `Bucket Name` to this list (editable in **Parameters** cell):\n",
    "\n",
    "```\n",
    "OKC, Atlanta, Orlando, Ontario, El Paso, Flowood, Phoenix, Charlotte\n",
    "```\n",
    "\n",
    "For each warehouse, labels are counted and sorted in descending order. We compute within-warehouse **percentage**\n",
    "and **cumulative percent**, and generate a Pareto chart (Count bars + Cumulative % line).\n",
    "\n",
    "> To avoid overly wide charts, the parameter `TOP_N_LABELS` limits each chart to the top N labels (default: 15). Set to `None` to show all.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9241b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# ----------------------------\n",
    "# Pareto - Labels by Warehouse (Bucket Name)\n",
    "# ----------------------------\n",
    "# Keep only the configured warehouses\n",
    "wh_mask = labels_exploded[\"Bucket Name\"].isin(WAREHOUSES)\n",
    "labels_by_wh = labels_exploded.loc[wh_mask].copy()\n",
    "labels_by_wh.rename(columns={\"Bucket Name\": \"Warehouse\"}, inplace=True)\n",
    "\n",
    "# Count labels per warehouse\n",
    "counts = (\n",
    "    labels_by_wh.groupby([\"Warehouse\", \"Labels\"])\n",
    "                .size()\n",
    "                .reset_index(name=\"Count\")\n",
    ")\n",
    "\n",
    "# Compute within-warehouse percent and cumulative percent\n",
    "counts[\"Percent\"] = counts.groupby(\"Warehouse\")[\"Count\"].transform(lambda x: (x / x.sum()) * 100)\n",
    "counts[\"Percent\"] = counts[\"Percent\"].round(2)\n",
    "\n",
    "# Sort and compute cumulative % per warehouse\n",
    "counts = counts.sort_values([\"Warehouse\", \"Count\"], ascending=[True, False])\n",
    "counts[\"Cumulative %\"] = (\n",
    "    counts.groupby(\"Warehouse\")[\"Count\"]\n",
    "          .transform(lambda x: x.cumsum() / x.sum() * 100)\n",
    "          .round(2)\n",
    ")\n",
    "\n",
    "# Save a copy for export\n",
    "labels_by_wh_table = counts.copy()\n",
    "\n",
    "# ----------------------------\n",
    "# Plot Pareto for each warehouse\n",
    "# ----------------------------\n",
    "def plot_pareto_for_warehouse(sub, warehouse, top_n=15):\n",
    "    sub_sorted = sub.sort_values(\"Count\", ascending=False)\n",
    "    if top_n is not None:\n",
    "        sub_sorted = sub_sorted.head(int(top_n))\n",
    "\n",
    "    labels = sub_sorted[\"Labels\"].tolist()\n",
    "    counts_ = sub_sorted[\"Count\"].tolist()\n",
    "    cum_pct = sub_sorted[\"Cumulative %\"].tolist()\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 5))  # one chart per figure (no subplots)\n",
    "    ax1 = plt.gca()\n",
    "    ax1.bar(labels, counts_)\n",
    "    ax1.set_xlabel(\"Label\")\n",
    "    ax1.set_ylabel(\"Count\")\n",
    "    ax1.set_title(f\"Pareto of Labels — {warehouse}\")\n",
    "\n",
    "    # Cumulative percent on a second y-axis\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(range(len(labels)), cum_pct, marker=\"o\")\n",
    "    ax2.set_ylabel(\"Cumulative %\")\n",
    "    ax2.set_ylim(0, 100)\n",
    "\n",
    "    # Rotate x labels for readability\n",
    "    ax1.tick_params(axis=\"x\", rotation=45, labelsize=9)\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Generate charts\n",
    "for wh in WAREHOUSES:\n",
    "    sub = counts[counts[\"Warehouse\"] == wh]\n",
    "    if not sub.empty:\n",
    "        plot_pareto_for_warehouse(sub, wh, top_n=TOP_N_LABELS)\n",
    "\n",
    "# Preview the first few rows of the by-warehouse table\n",
    "labels_by_wh_table.head(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95daf79",
   "metadata": {},
   "source": [
    "## Export for Power BI\n",
    "\n",
    "The following tables are written to `Claims_Performance_Summary.xlsx`:\n",
    "- `Bucket_Performance`\n",
    "- `Pareto_Labels_Overall`\n",
    "- `Pareto_Labels_By_Warehouse` (long format: Warehouse, Label, Count, Percent, Cumulative %)\n",
    "\n",
    "You can import these sheets directly into **Power BI** (Get Data → Excel). Because tables are rebuilt on each run, you can refresh your Power BI model as data updates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417195f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# ----------------------------\n",
    "# Export to Excel for Power BI\n",
    "# ----------------------------\n",
    "with pd.ExcelWriter(OUTPUT_XLSX, engine=\"openpyxl\") as writer:\n",
    "    bucket_perf.to_excel(writer, sheet_name=\"Bucket_Performance\", index=False)\n",
    "    labels_overall.to_excel(writer, sheet_name=\"Pareto_Labels_Overall\", index=False)\n",
    "    labels_by_wh_table.to_excel(writer, sheet_name=\"Pareto_Labels_By_Warehouse\", index=False)\n",
    "\n",
    "print(f\"Exported Power BI workbook -> {OUTPUT_XLSX.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597fd910",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "- Connect `Claims_Performance_Summary.xlsx` to **Power BI** and build visuals from the exported tables.\n",
    "- Optionally schedule automatic refresh tied to a data export process (e.g., Planner/Service tool → Excel) so your dashboard stays current.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
