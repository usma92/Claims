{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Claims — Closed Summary & Open Aging Pareto (v2)\nGenerated: 2025-11-05 19:16:32\n\nThis notebook:\n- Lets you pick **warehouses**, **date range** and **max labels**.\n- Builds a **Closed Claims** summary (count, mean, median, min, max days to close).\n- Builds **Open Claims Aging** buckets and makes a **Pareto** (bar + cumulative %) per warehouse.\n- **Merges both files** into one **combined DataFrame** with **all original columns**, plus standardized columns appended.\n- Exports to `claims_analysis_output.xlsx` with three sheets:\n  - `All_Claims_Combined` (merged; filtered to selected warehouses/date)\n  - `Closed_Claims_Summary`\n  - `Open_Claims_Aging`\n\n**Definitions**\n- **Closed** = `Completed Date` **present**\n- **Open** = `Completed Date` **blank**\n- **Days to close** = `Completed Date - Start Date` (calendar days)\n- **Aging buckets** for open claims: `<30`, `30–<60`, `60–<90`, `≥90`"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ==== Parameters (edit these) ====\nwarehouses = [\"OKC\", \"Atlanta\", \"Orlando\", \"Ontario\", \"El Paso\", \"Flowood\", \"Phoenix\", \"Charlotte\"]\n\n# Date window applied to Start Date. Set None to disable either bound.\nstart_date = None   # e.g., \"2024-01-01\"\nend_date   = None   # e.g., \"2025-12-31\"\n\n# Max number of bars to annotate with labels in each Pareto chart\nmax_labels = 25\n\n# Input files expected in the same folder as this notebook (adjust paths if needed)\nINPUT_FILES = [\n    (\"Claims.xlsx\", \"Claims\"),\n    (\"Chrysler Claims.xlsx\", \"Chrysler Claims\")\n]\n\n# Output locations\nOUT_XLSX = \"claims_analysis_output.xlsx\"\nFIG_DIR = \"figs_claims\""
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ==== Imports ====\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import MaxNLocator, PercentFormatter\nfrom pathlib import Path\nimport re\nfrom typing import Optional, Tuple, Dict\n\n# Create the figure directory if needed\nPath(FIG_DIR).mkdir(parents=True, exist_ok=True)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ==== Helpers ====\n\ndef safe_filename(name: str, max_len: int = 180) -> str:\n    \"\"\"Sanitize a string for use as a filename on Windows/macOS/Linux.\"\"\"\n    if name is None:\n        name = \"untitled\"\n    name = re.sub(r'[<>:\"/\\\\|?*]', \"_\", str(name))\n    name = re.sub(r\"[\\x00-\\x1f]\", \"_\", name)\n    name = re.sub(r\"_+\", \"_\", name).strip(\" .\")\n    if not name:\n        name = \"unnamed\"\n    return name[:max_len]\n\ndef find_column(df: pd.DataFrame, candidates):\n    cols = {c.lower(): c for c in df.columns}\n    for cand in candidates:\n        if cand.lower() in cols:\n            return cols[cand.lower()]\n    return None\n\ndef normalize_columns(df: pd.DataFrame) -> Tuple[pd.DataFrame, Dict[str, str]]:\n    \"\"\"Return df standardized to columns: Warehouse, Start Date, Completed Date.\"\"\"\n    mapping = {}\n    wh_col = find_column(df, [\"Bucket Name\", \"Warehouse\", \"WH\", \"Facility\"])\n    sd_col = find_column(df, [\"Start Date\", \"Open Date\", \"Opened Date\", \"Created Date\", \"Create Date\", \"Claim Open Date\", \"Date Opened\"])\n    cd_col = find_column(df, [\"Completed Date\", \"Close Date\", \"Closed Date\"])\n\n    if wh_col is None:\n        raise KeyError(\"Could not find a warehouse column (tried: Bucket Name, Warehouse, WH, Facility).\")\n    if sd_col is None:\n        raise KeyError(\"Could not find a Start/Open date column (tried common variants).\")\n\n    out = df.copy()\n    out.rename(columns={wh_col: \"Warehouse\"}, inplace=True)\n    out.rename(columns={sd_col: \"Start Date\"}, inplace=True)\n\n    if cd_col is not None:\n        out.rename(columns={cd_col: \"Completed Date\"}, inplace=True)\n    else:\n        if \"Completed Date\" not in out.columns:\n            out[\"Completed Date\"] = pd.NaT\n\n    # Parse dates\n    out[\"Start Date\"] = pd.to_datetime(out[\"Start Date\"], errors=\"coerce\")\n    out[\"Completed Date\"] = pd.to_datetime(out[\"Completed Date\"], errors=\"coerce\")\n\n    # Clean warehouse values\n    out[\"Warehouse\"] = out[\"Warehouse\"].astype(str).str.strip()\n    return out, mapping\n\ndef filter_by_warehouses_and_dates(df: pd.DataFrame,\n                                   warehouses: list,\n                                   start_date: Optional[str],\n                                   end_date: Optional[str]) -> pd.DataFrame:\n    # Warehouse filter (case-insensitive)\n    wanted = {w.strip().lower(): w for w in warehouses}\n    df2 = df[df[\"Warehouse\"].str.lower().isin(wanted.keys())].copy()\n    df2[\"Warehouse\"] = df2[\"Warehouse\"].str.lower().map(wanted)\n\n    # Date range filter on Start Date\n    if start_date:\n        df2 = df2[df2[\"Start Date\"] >= pd.to_datetime(start_date)]\n    if end_date:\n        df2 = df2[df2[\"Start Date\"] <= pd.to_datetime(end_date)]\n    return df2\n\ndef closed_claims_summary(df: pd.DataFrame, dataset_name: str) -> pd.DataFrame:\n    # Closed = Completed Date present\n    closed = df[df[\"Completed Date\"].notna()].copy()\n    closed[\"DaysToClose\"] = (closed[\"Completed Date\"] - closed[\"Start Date\"]).dt.days\n\n    agg = closed.groupby(\"Warehouse\", dropna=False)[\"DaysToClose\"].agg(\n        CountClosed=\"count\",\n        MeanDaysToClose=lambda s: float(np.nanmean(s)) if len(s) else np.nan,\n        MedianDaysToClose=lambda s: float(np.nanmedian(s)) if len(s) else np.nan,\n        MinDaysToClose=lambda s: float(np.nanmin(s)) if len(s) else np.nan,\n        MaxDaysToClose=lambda s: float(np.nanmax(s)) if len(s) else np.nan,\n    ).reset_index()\n\n    agg.insert(0, \"Dataset\", dataset_name)\n    return agg\n\ndef open_claims_aging_table(df: pd.DataFrame, dataset_name: str) -> pd.DataFrame:\n    # Open = Completed Date blank\n    open_df = df[df[\"Completed Date\"].isna()].copy()\n\n    # Age days = today - Start Date\n    today = pd.Timestamp.today().normalize()\n    open_df[\"AgeDays\"] = (today - open_df[\"Start Date\"].dt.normalize()).dt.days\n\n    # Buckets\n    bucket_labels = [\"<30 days\", \"30-<60 days\", \"60-<90 days\", \">= 90 days\"]\n    bins = [-1, 29, 59, 89, float(\"inf\")]\n    open_df[\"Aging Bucket\"] = pd.cut(open_df[\"AgeDays\"], bins=bins, labels=bucket_labels, include_lowest=True, right=True)\n\n    # Counts by warehouse & bucket\n    counts = (open_df\n              .groupby([\"Warehouse\", \"Aging Bucket\"])\n              .size()\n              .rename(\"Count\")\n              .reset_index())\n\n    # Reindex to ensure all buckets for each warehouse (in user-specified order)\n    all_wh = [w for w in warehouses if w in counts[\"Warehouse\"].unique().tolist()]\n    full_index = pd.MultiIndex.from_product([all_wh, bucket_labels], names=[\"Warehouse\", \"Aging Bucket\"])\n    counts = counts.set_index([\"Warehouse\", \"Aging Bucket\"]).reindex(full_index, fill_value=0).reset_index()\n\n    # Percent + CumPercent\n    counts[\"Percent\"] = counts.groupby(\"Warehouse\")[\"Count\"].transform(lambda x: (x / x.sum() * 100.0) if x.sum() else 0.0)\n    counts[\"CumPercent\"] = counts.groupby(\"Warehouse\")[\"Percent\"].cumsum()\n\n    counts.insert(0, \"Dataset\", dataset_name)\n    return counts\n\ndef plot_pareto_for_wh(df_aging_wh: pd.DataFrame, dataset_name: str, wh: str, max_labels: int = 25, save_dir: str = FIG_DIR):\n    labels = df_aging_wh[\"Aging Bucket\"].astype(str).tolist()\n    x = np.arange(len(labels))\n    counts = df_aging_wh[\"Count\"].to_numpy(dtype=float)\n    cum_pct = df_aging_wh[\"CumPercent\"].to_numpy(dtype=float)\n\n    fig, ax = plt.subplots(figsize=(8, 4.6))\n    bars = ax.bar(x, counts)\n    ax.set_xticks(x)\n    ax.set_xticklabels(labels, rotation=0)\n    ax.set_xlabel(\"Claim Age\")\n    ax.set_ylabel(\"Open Claims (count)\")\n    ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n\n    ax2 = ax.twinx()\n    ax2.plot(x, cum_pct, marker=\"o\")\n    ax2.set_ylabel(\"Cumulative %\")\n    ax2.set_ylim(0, 100)\n    ax2.yaxis.set_major_formatter(PercentFormatter(xmax=100))\n\n    total = int(np.nansum(counts))\n    ax.set_title(f\"{dataset_name} — Open Claims Aging — {wh} (n={total})\")\n    ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.3)\n    fig.tight_layout()\n\n    # annotate if few enough bars\n    if len(labels) <= max_labels:\n        for rect, val in zip(bars, counts):\n            ax.text(rect.get_x() + rect.get_width()/2, rect.get_height(), f\"{int(val)}\",\n                    ha=\"center\", va=\"bottom\", fontsize=9)\n\n    # Save\n    Path(save_dir).mkdir(parents=True, exist_ok=True)\n    fname = f\"{safe_filename(dataset_name)}__aging_pareto__{safe_filename(wh)}.png\"\n    fig.savefig(Path(save_dir)/fname, dpi=150, bbox_inches=\"tight\")\n    plt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ==== Load files, build combined (raw+standardized), filter, compute tables ====\ncombined_rows = []           # merged raw + standardized + Dataset for BOTH files\nclosed_summaries = []\nopen_aging_tables = []\n\nfor rel_path, dname in INPUT_FILES:\n    path = Path(rel_path)\n    if not path.exists():\n        print(f\"WARNING: File not found, skipping: {rel_path}\")\n        continue\n\n    # raw data (all original columns)\n    raw = pd.read_excel(path)\n    # standardized for analysis\n    df_std, _ = normalize_columns(raw)\n\n    # combined (raw + standardized columns + Dataset)\n    df_comb = raw.copy()\n    for col in [\"Warehouse\", \"Start Date\", \"Completed Date\"]:\n        # add standardized columns alongside raw\n        if col in df_std.columns:\n            df_comb[col] = df_std[col]\n    df_comb[\"Dataset\"] = dname\n    combined_rows.append(df_comb)\n\n    # Filter standardized for analysis\n    df_std_f = filter_by_warehouses_and_dates(df_std, warehouses, start_date, end_date)\n\n    # Summaries\n    closed_summaries.append(closed_claims_summary(df_std_f, dname))\n    open_aging_tables.append(open_claims_aging_table(df_std_f, dname))\n\n# Merged combined (raw+standardized) across datasets, then filter by Warehouse/Start Date for output parity\ncombined_all = pd.concat(combined_rows, ignore_index=True) if combined_rows else pd.DataFrame()\n\n# Apply the same filter to the combined set for output (based on standardized columns we just added)\nif not combined_all.empty:\n    # Drop records missing standardized Warehouse/Start Date if those were not present in raw\n    mask = combined_all[\"Warehouse\"].notna()\n    if start_date:\n        mask &= pd.to_datetime(combined_all[\"Start Date\"], errors=\"coerce\") >= pd.to_datetime(start_date)\n    if end_date:\n        mask &= pd.to_datetime(combined_all[\"Start Date\"], errors=\"coerce\") <= pd.to_datetime(end_date)\n    # Warehouse filter (case-insensitive)\n    wanted = {w.strip().lower() for w in warehouses}\n    mask &= combined_all[\"Warehouse\"].astype(str).str.lower().isin(wanted)\n    combined_all_filtered = combined_all[mask].copy()\nelse:\n    combined_all_filtered = combined_all\n\n# Compute aggregated outputs\nclosed_all = pd.concat(closed_summaries, ignore_index=True) if closed_summaries else pd.DataFrame()\naging_all  = pd.concat(open_aging_tables, ignore_index=True) if open_aging_tables else pd.DataFrame()\n\n# Peek\ncombined_all_filtered.head(20), closed_all.head(10), aging_all.head(10)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ==== Export to Excel ====\nwith pd.ExcelWriter(OUT_XLSX, engine=\"openpyxl\") as writer:\n    if not combined_all_filtered.empty:\n        combined_all_filtered.to_excel(writer, sheet_name=\"All_Claims_Combined\", index=False)\n    if not closed_all.empty:\n        closed_all.to_excel(writer, sheet_name=\"Closed_Claims_Summary\", index=False)\n    if not aging_all.empty:\n        aging_all.to_excel(writer, sheet_name=\"Open_Claims_Aging\", index=False)\n\nprint(f\"Wrote: {OUT_XLSX}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ==== Generate Pareto charts per dataset & warehouse ====\nif not aging_all.empty:\n    order = [\"<30 days\", \"30-<60 days\", \"60-<90 days\", \">= 90 days\"]\n    for dname in aging_all[\"Dataset\"].unique():\n        sub_d = aging_all[aging_all[\"Dataset\"] == dname]\n        for wh in warehouses:\n            wh_rows = sub_d[sub_d[\"Warehouse\"] == wh]\n            if wh_rows.empty:\n                continue\n            wh_rows = wh_rows.set_index(\"Aging Bucket\").reindex(order).reset_index()\n            plot_pareto_for_wh(wh_rows, dname, wh, max_labels=max_labels, save_dir=FIG_DIR)\nelse:\n    print(\"No aging data to plot.\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}