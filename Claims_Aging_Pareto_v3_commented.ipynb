{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Claims — Closed Summary, Open Aging Pareto & Open Tasks (v3 — Commented)\n**Generated:** 2025-11-05 20:53:15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Parameters\nwarehouses = [\"OKC\", \"Atlanta\", \"Orlando\", \"Ontario\", \"El Paso\", \"Flowood\", \"Phoenix\", \"Charlotte\"]\nstart_date = None\nend_date   = None\nmax_labels = 25\nINPUT_FILES = [(\"Claims.xlsx\", \"Claims\"), (\"Chrysler Claims.xlsx\", \"Chrysler Claims\")]\nOUT_XLSX = \"claims_analysis_output.xlsx\"\nFIG_DIR = \"figs_claims\""
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Imports\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import MaxNLocator, PercentFormatter\nfrom pathlib import Path\nimport re\nfrom typing import Optional, Tuple, Dict\nPath(FIG_DIR).mkdir(parents=True, exist_ok=True)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Helpers\ndef safe_filename(name: str, max_len: int = 180) -> str:\n    if name is None:\n        name = \"untitled\"\n    name = re.sub(r'[<>:\"/\\\\|?*]', \"_\", str(name))\n    name = re.sub(r\"[\\x00-\\x1f]\", \"_\", name)\n    name = re.sub(r\"_+\", \"_\", name).strip(\" .\")\n    if not name:\n        name = \"unnamed\"\n    return name[:max_len]\n\ndef find_column(df: pd.DataFrame, candidates):\n    cols = {c.lower(): c for c in df.columns}\n    for cand in candidates:\n        if cand.lower() in cols:\n            return cols[cand.lower()]\n    return None\n\ndef normalize_columns(df: pd.DataFrame):\n    wh_col = find_column(df, [\"Bucket Name\", \"Warehouse\", \"WH\", \"Facility\"])\n    sd_col = find_column(df, [\"Start Date\", \"Open Date\", \"Opened Date\", \"Created Date\", \"Create Date\", \"Claim Open Date\", \"Date Opened\"])\n    cd_col = find_column(df, [\"Completed Date\", \"Close Date\", \"Closed Date\"])\n    task_col = find_column(df, [\"Task Name\", \"Task\"])\n    labels_col = find_column(df, [\"Labels\", \"Label\"])\n    due_col = find_column(df, [\"Due Date\", \"Due date\", \"Task Due\", \"Due\"])\n\n    if wh_col is None:\n        raise KeyError(\"Warehouse column not found\")\n    if sd_col is None:\n        raise KeyError(\"Start/Open date column not found\")\n\n    out = df.copy()\n    out.rename(columns={wh_col: \"Warehouse\", sd_col: \"Start Date\"}, inplace=True)\n    if cd_col is not None: out.rename(columns={cd_col: \"Completed Date\"}, inplace=True)\n    else: out[\"Completed Date\"] = pd.NaT\n    if task_col is not None: out.rename(columns={task_col: \"Task Name\"}, inplace=True)\n    else: out[\"Task Name\"] = \"\"\n    if labels_col is not None: out.rename(columns={labels_col: \"Labels\"}, inplace=True)\n    else: out[\"Labels\"] = \"\"\n    if due_col is not None: out.rename(columns={due_col: \"Due Date\"}, inplace=True)\n    else: out[\"Due Date\"] = pd.NaT\n\n    out[\"Start Date\"] = pd.to_datetime(out[\"Start Date\"], errors=\"coerce\")\n    out[\"Completed Date\"] = pd.to_datetime(out[\"Completed Date\"], errors=\"coerce\")\n    out[\"Due Date\"] = pd.to_datetime(out[\"Due Date\"], errors=\"coerce\")\n    out[\"Warehouse\"] = out[\"Warehouse\"].astype(str).str.strip()\n    out[\"Task Name\"] = out[\"Task Name\"].astype(str).str.strip()\n    out[\"Labels\"] = out[\"Labels\"].astype(str)\n    return out, {}"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def filter_by_warehouses_and_dates(df, warehouses, start_date, end_date):\n    wanted = {w.strip().lower(): w for w in warehouses}\n    df2 = df[df[\"Warehouse\"].str.lower().isin(wanted.keys())].copy()\n    df2[\"Warehouse\"] = df2[\"Warehouse\"].str.lower().map(wanted)\n    if start_date:\n        df2 = df2[df2[\"Start Date\"] >= pd.to_datetime(start_date)]\n    if end_date:\n        df2 = df2[df2[\"Start Date\"] <= pd.to_datetime(end_date)]\n    return df2\n\ndef closed_claims_summary(df, dataset_name):\n    closed = df[df[\"Completed Date\"].notna()].copy()\n    closed[\"DaysToClose\"] = (closed[\"Completed Date\"] - closed[\"Start Date\"]).dt.days\n    agg = closed.groupby(\"Warehouse\", dropna=False)[\"DaysToClose\"].agg(\n        CountClosed=\"count\",\n        MeanDaysToClose=lambda s: float(np.nanmean(s)) if len(s) else np.nan,\n        MedianDaysToClose=lambda s: float(np.nanmedian(s)) if len(s) else np.nan,\n        MinDaysToClose=lambda s: float(np.nanmin(s)) if len(s) else np.nan,\n        MaxDaysToClose=lambda s: float(np.nanmax(s)) if len(s) else np.nan,\n    ).reset_index()\n    agg.insert(0, \"Dataset\", dataset_name)\n    return agg\n\ndef open_claims_aging_table(df, dataset_name):\n    open_df = df[df[\"Completed Date\"].isna()].copy()\n    today = pd.Timestamp.today().normalize()\n    open_df[\"AgeDays\"] = (today - open_df[\"Start Date\"].dt.normalize()).dt.days\n    bucket_labels = [\"<30 days\", \"30-<60 days\", \"60-<90 days\", \">= 90 days\"]\n    bins = [-1, 29, 59, 89, float(\"inf\")]\n    open_df[\"Aging Bucket\"] = pd.cut(open_df[\"AgeDays\"], bins=bins, labels=bucket_labels, include_lowest=True, right=True)\n    counts = (open_df.groupby([\"Warehouse\", \"Aging Bucket\"]).size().rename(\"Count\").reset_index())\n    all_wh = [w for w in warehouses if w in counts[\"Warehouse\"].unique().tolist()]\n    full_index = pd.MultiIndex.from_product([all_wh, bucket_labels], names=[\"Warehouse\", \"Aging Bucket\"])\n    counts = counts.set_index([\"Warehouse\", \"Aging Bucket\"]).reindex(full_index, fill_value=0).reset_index()\n    counts[\"Percent\"] = counts.groupby(\"Warehouse\")[\"Count\"].transform(lambda x: (x / x.sum() * 100.0) if x.sum() else 0.0)\n    counts[\"CumPercent\"] = counts.groupby(\"Warehouse\")[\"Percent\"].cumsum()\n    counts.insert(0, \"Dataset\", dataset_name)\n    return counts\n\ndef plot_pareto_for_wh(df_aging_wh, dataset_name, wh, max_labels=25, save_dir=FIG_DIR):\n    labels = df_aging_wh[\"Aging Bucket\"].astype(str).tolist()\n    x = np.arange(len(labels))\n    counts = df_aging_wh[\"Count\"].to_numpy(dtype=float)\n    cum_pct = df_aging_wh[\"CumPercent\"].to_numpy(dtype=float)\n    fig, ax = plt.subplots(figsize=(8, 4.6))\n    bars = ax.bar(x, counts)\n    ax.set_xticks(x); ax.set_xticklabels(labels, rotation=0)\n    ax.set_xlabel(\"Claim Age\"); ax.set_ylabel(\"Open Claims (count)\")\n    ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n    ax2 = ax.twinx(); ax2.plot(x, cum_pct, marker=\"o\"); ax2.set_ylabel(\"Cumulative %\")\n    ax2.set_ylim(0, 100); ax2.yaxis.set_major_formatter(PercentFormatter(xmax=100))\n    total = int(np.nansum(counts))\n    ax.set_title(f\"{dataset_name} — Open Claims Aging — {wh} (n={total})\")\n    ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.3); fig.tight_layout()\n    if len(labels) <= max_labels:\n        for rect, val in zip(bars, counts):\n            ax.text(rect.get_x()+rect.get_width()/2, rect.get_height(), f\"{int(val)}\", ha=\"center\", va=\"bottom\", fontsize=9)\n    Path(save_dir).mkdir(parents=True, exist_ok=True)\n    fname = f\"{safe_filename(dataset_name)}__aging_pareto__{safe_filename(wh)}.png\"\n    fig.savefig(Path(save_dir)/fname, dpi=150, bbox_inches=\"tight\"); plt.show()\n\ndef extract_wh_actions(labels_value: str):\n    parts = re.split(r\"[;|,]\", str(labels_value))\n    wh_only = []\n    for p in parts:\n        s = p.strip()\n        if re.match(r\"(?i)^WH\\b\", s):\n            wh_only.append(s)\n    return wh_only"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load/standardize/combine/compute\ncombined_rows = []\nclosed_summaries = []\nopen_aging_tables = []\n\nfor rel_path, dname in INPUT_FILES:\n    path = Path(rel_path)\n    if not path.exists():\n        print(f\"WARNING: File not found, skipping: {rel_path}\")\n        continue\n    raw = pd.read_excel(path)\n    df_std, _ = normalize_columns(raw)\n    df_comb = raw.copy()\n    for col in [\"Warehouse\", \"Start Date\", \"Completed Date\", \"Task Name\", \"Labels\", \"Due Date\"]:\n        if col in df_std.columns:\n            df_comb[col] = df_std[col]\n    df_comb[\"Dataset\"] = dname\n    combined_rows.append(df_comb)\n    df_std_f = filter_by_warehouses_and_dates(df_std, warehouses, start_date, end_date)\n    closed_summaries.append(closed_claims_summary(df_std_f, dname))\n    open_aging_tables.append(open_claims_aging_table(df_std_f, dname))\n\ncombined_all = pd.concat(combined_rows, ignore_index=True) if combined_rows else pd.DataFrame()\nif not combined_all.empty:\n    mask = combined_all[\"Warehouse\"].notna()\n    if start_date:\n        mask &= pd.to_datetime(combined_all[\"Start Date\"], errors=\"coerce\") >= pd.to_datetime(start_date)\n    if end_date:\n        mask &= pd.to_datetime(combined_all[\"Start Date\"], errors=\"coerce\") <= pd.to_datetime(end_date)\n    wanted = {w.strip().lower() for w in warehouses}\n    mask &= combined_all[\"Warehouse\"].astype(str).str.lower().isin(wanted)\n    combined_all_filtered = combined_all[mask].copy()\nelse:\n    combined_all_filtered = combined_all\n\nclosed_all = pd.concat(closed_summaries, ignore_index=True) if closed_summaries else pd.DataFrame()\naging_all  = pd.concat(open_aging_tables, ignore_index=True) if open_aging_tables else pd.DataFrame()\ncombined_all_filtered.head(10), closed_all.head(5), aging_all.head(5)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Open Tasks – detail & aggregate\nopen_task_rows = []\nfor rel_path, dname in INPUT_FILES:\n    path = Path(rel_path)\n    if not path.exists():\n        continue\n    raw = pd.read_excel(path)\n    df_std, _ = normalize_columns(raw)\n    df_std_f = filter_by_warehouses_and_dates(df_std, warehouses, start_date, end_date)\n    open_df = df_std_f[df_std_f[\"Completed Date\"].isna()].copy()\n    open_df[\"ActionList\"] = open_df[\"Labels\"].apply(extract_wh_actions)\n    open_df = open_df[open_df[\"ActionList\"].map(len) > 0].copy().explode(\"ActionList\")\n    open_df.rename(columns={\"ActionList\":\"Action Required\"}, inplace=True)\n    open_df[\"Dataset\"] = dname\n    open_task_rows.append(open_df[[\"Dataset\",\"Warehouse\",\"Task Name\",\"Action Required\",\"Due Date\"]])\n\nopen_tasks_detail = pd.concat(open_task_rows, ignore_index=True) if open_task_rows else pd.DataFrame(columns=[\"Dataset\",\"Warehouse\",\"Task Name\",\"Action Required\",\"Due Date\"])\nif not open_tasks_detail.empty:\n    open_tasks_detail = open_tasks_detail.sort_values([\"Action Required\",\"Due Date\",\"Warehouse\"], na_position=\"last\").reset_index(drop=True)\n\nif not open_tasks_detail.empty:\n    open_tasks_agg = (open_tasks_detail.groupby([\"Dataset\",\"Warehouse\",\"Action Required\"], dropna=False).size().rename(\"Count\").reset_index()\n                      .sort_values([\"Warehouse\",\"Count\"], ascending=[True, False], ignore_index=True))\nelse:\n    open_tasks_agg = pd.DataFrame(columns=[\"Dataset\",\"Warehouse\",\"Action Required\",\"Count\"])\n\nopen_tasks_detail.head(10), open_tasks_agg.head(10)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Export all outputs to Excel (overwrites file each run to avoid stale data)\nwith pd.ExcelWriter(OUT_XLSX, engine=\"openpyxl\") as writer:\n    if not combined_all_filtered.empty:\n        combined_all_filtered.to_excel(writer, sheet_name=\"All_Claims_Combined\", index=False)\n    if not closed_all.empty:\n        closed_all.to_excel(writer, sheet_name=\"Closed_Claims_Summary\", index=False)\n    if not aging_all.empty:\n        aging_all.to_excel(writer, sheet_name=\"Open_Claims_Aging\", index=False)\n    if not open_tasks_detail.empty:\n        open_tasks_detail.to_excel(writer, sheet_name=\"Open_Tasks_By_Warehouse\", index=False)\n    if not open_tasks_agg.empty:\n        open_tasks_agg.to_excel(writer, sheet_name=\"Open_Tasks_Aggregate\", index=False)\nprint(f\"Wrote: {OUT_XLSX}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Pareto chart generation\nif not aging_all.empty:\n    desired_order = [\"<30 days\", \"30-<60 days\", \"60-<90 days\", \">= 90 days\"]\n    for dname in aging_all[\"Dataset\"].unique():\n        sub = aging_all[aging_all[\"Dataset\"] == dname]\n        for wh in warehouses:\n            wh_rows = sub[sub[\"Warehouse\"] == wh]\n            if wh_rows.empty:\n                continue\n            wh_rows = wh_rows.set_index(\"Aging Bucket\").reindex(desired_order).reset_index()\n            plot_pareto_for_wh(wh_rows, dname, wh, max_labels=max_labels, save_dir=FIG_DIR)\nelse:\n    print(\"No aging data to plot.\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}