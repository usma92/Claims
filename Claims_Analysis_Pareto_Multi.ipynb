{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3a4a99e",
   "metadata": {},
   "source": [
    "# Claims Performance Analysis (Multi-File) — With Date Range Filter\n",
    "\n",
    "This notebook ingests multiple **dealer claims** workbooks and performs the same analysis for each, plus a **combined view**.\n",
    "It now includes a **date range filter** you can apply to **Created Date**, **Completed Date**, or **Either**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7152b313",
   "metadata": {},
   "source": [
    "## Inputs\n",
    "- Default files (edit in **Parameters**):\n",
    "  - `Claims.xlsx`\n",
    "  - `Chrysler Claims.xlsx`\n",
    "- Expected sheet: `Tasks`\n",
    "- Expected columns: `Bucket Name`, `Labels`, `Created Date`, `Completed Date`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8725803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 50)\n",
    "pd.set_option(\"display.max_columns\", 120)\n",
    "pd.set_option(\"display.width\", 140)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c0d462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# ----------------------------\n",
    "# Parameters (EDITABLE)\n",
    "# ----------------------------\n",
    "INPUT_FILES = [\n",
    "    {\"name\": \"Claims\", \"filename\": \"Claims.xlsx\"},\n",
    "    {\"name\": \"Chrysler Claims\", \"filename\": \"Chrysler Claims.xlsx\"},\n",
    "]\n",
    "\n",
    "SHEET_NAME = \"Tasks\"\n",
    "OUTPUT_XLSX = Path(\"Claims_Performance_Summary_ALL.xlsx\")\n",
    "\n",
    "# Warehouses for by-warehouse Pareto (edit as needed)\n",
    "WAREHOUSES = [\"OKC\", \"Atlanta\", \"Orlando\", \"Ontario\", \"El Paso\", \"Flowood\", \"Phoenix\", \"Charlotte\"]\n",
    "\n",
    "# Max labels to show in each Pareto chart\n",
    "TOP_N_LABELS = 15  # None to show all\n",
    "\n",
    "# ----------------------------\n",
    "# Date Range Filter\n",
    "# ----------------------------\n",
    "# Filter mode: \"Created\", \"Completed\", or \"Either\"\n",
    "DATE_FILTER_MODE = \"Created\"\n",
    "\n",
    "# Provide dates as 'YYYY-MM-DD' strings or None to disable.\n",
    "# If only START_DATE is provided, filter from that date forward (inclusive).\n",
    "# If only END_DATE is provided, filter up to that date (inclusive).\n",
    "START_DATE = None      # e.g., \"2024-01-01\"\n",
    "END_DATE = None        # e.g., \"2024-12-31\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6be45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# ----------------------------\n",
    "# Helper functions\n",
    "# ----------------------------\n",
    "def resolve_path(fname: str) -> Path:\n",
    "    p = Path(fname)\n",
    "    if p.exists():\n",
    "        return p\n",
    "    fallback = Path(\"/mnt/data\") / fname\n",
    "    if fallback.exists():\n",
    "        return fallback\n",
    "    raise FileNotFoundError(f\"Could not find input Excel '{fname}' at {p.resolve()} or {fallback.resolve()}\")\n",
    "\n",
    "def load_and_prepare(path: Path, sheet: str, dataset_name: str) -> pd.DataFrame:\n",
    "    xls = pd.ExcelFile(path)\n",
    "    if sheet not in xls.sheet_names:\n",
    "        raise ValueError(f\"Sheet '{sheet}' not found in {path.name}. Available: {xls.sheet_names}\")\n",
    "    df = pd.read_excel(path, sheet_name=sheet)\n",
    "\n",
    "    required = [\"Bucket Name\", \"Labels\", \"Created Date\", \"Completed Date\"]\n",
    "    for col in required:\n",
    "        if col not in df.columns:\n",
    "            raise ValueError(f\"Expected column '{col}' not found in {path.name}/{sheet}\")\n",
    "    df[\"Bucket Name\"] = df[\"Bucket Name\"].astype(str).str.strip()\n",
    "    df[\"Labels\"] = df[\"Labels\"].astype(str).str.strip()\n",
    "    df[\"Created Date\"] = pd.to_datetime(df[\"Created Date\"], errors=\"coerce\")\n",
    "    df[\"Completed Date\"] = pd.to_datetime(df[\"Completed Date\"], errors=\"coerce\")\n",
    "\n",
    "    # Cycle time\n",
    "    df[\"Cycle Time (Days)\"] = (df[\"Completed Date\"] - df[\"Created Date\"]).dt.days\n",
    "\n",
    "    # Dataset tag\n",
    "    df[\"Dataset\"] = dataset_name\n",
    "    return df\n",
    "\n",
    "def apply_date_filter(df: pd.DataFrame, mode: str, start_str, end_str) -> pd.DataFrame:\n",
    "    \"\"\"Filter rows by Created/Completed/Either date within [start, end] inclusive.\n",
    "    - mode: 'Created', 'Completed', or 'Either'\n",
    "    - start_str/end_str: 'YYYY-MM-DD' or None\n",
    "    \"\"\"\n",
    "    if not start_str and not end_str:\n",
    "        return df.copy()\n",
    "\n",
    "    start = pd.to_datetime(start_str) if start_str else None\n",
    "    end = pd.to_datetime(end_str) if end_str else None\n",
    "\n",
    "    created = df[\"Created Date\"]\n",
    "    completed = df[\"Completed Date\"]\n",
    "\n",
    "    def in_range(series):\n",
    "        mask = pd.Series(True, index=series.index)\n",
    "        if start is not None:\n",
    "            mask &= series >= start\n",
    "        if end is not None:\n",
    "            # inclusive upper bound\n",
    "            mask &= series <= end\n",
    "        return mask\n",
    "\n",
    "    if mode.lower().startswith(\"created\"):\n",
    "        mask = in_range(created)\n",
    "    elif mode.lower().startswith(\"completed\"):\n",
    "        mask = in_range(completed)\n",
    "    else:  # 'Either' — created in range OR completed in range\n",
    "        mask = in_range(created)\n",
    "        mask |= in_range(completed)\n",
    "    return df[mask].copy()\n",
    "\n",
    "def explode_labels(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df[[\"Dataset\", \"Bucket Name\", \"Labels\"]].copy()\n",
    "    out = out.assign(Labels=out[\"Labels\"].str.split(\";\")).explode(\"Labels\")\n",
    "    out[\"Labels\"] = out[\"Labels\"].astype(str).str.strip()\n",
    "    out = out[out[\"Labels\"].ne(\"\")]\n",
    "    return out\n",
    "\n",
    "def compute_bucket_perf(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    agg = (\n",
    "        df.groupby([\"Dataset\", \"Bucket Name\"])[\"Cycle Time (Days)\"]\n",
    "          .agg([\"count\", \"mean\", \"median\", \"min\", \"max\"])\n",
    "          .reset_index()\n",
    "    )\n",
    "    agg.rename(columns={\n",
    "        \"count\": \"Count\",\n",
    "        \"mean\": \"Mean (days)\",\n",
    "        \"median\": \"Median (days)\",\n",
    "        \"min\": \"Min (days)\",\n",
    "        \"max\": \"Max (days)\"\n",
    "    }, inplace=True)\n",
    "    agg[\"Mean (days)\"] = agg[\"Mean (days)\"].round(2)\n",
    "    agg[\"Median (days)\"] = agg[\"Median (days)\"].round(2)\n",
    "    return agg\n",
    "\n",
    "def compute_overall_pareto(labels_exploded: pd.DataFrame) -> pd.DataFrame:\n",
    "    vc = labels_exploded[\"Labels\"].value_counts().rename_axis(\"Label\").reset_index(name=\"Count\")\n",
    "    total = vc[\"Count\"].sum()\n",
    "    vc[\"Cumulative %\"] = (vc[\"Count\"].cumsum() / total * 100).round(2)\n",
    "    vc.insert(0, \"Dataset\", \"ALL\")\n",
    "    return vc\n",
    "\n",
    "def compute_by_wh_pareto(labels_exploded: pd.DataFrame, warehouses: list[str]) -> pd.DataFrame:\n",
    "    wh = labels_exploded[labels_exploded[\"Bucket Name\"].isin(warehouses)].copy()\n",
    "    wh.rename(columns={\"Bucket Name\": \"Warehouse\"}, inplace=True)\n",
    "    counts = (\n",
    "        wh.groupby([\"Dataset\", \"Warehouse\", \"Labels\"])\n",
    "          .size()\n",
    "          .reset_index(name=\"Count\")\n",
    "    )\n",
    "    counts[\"Percent\"] = counts.groupby([\"Dataset\", \"Warehouse\"])[\"Count\"].transform(lambda x: x / x.sum() * 100).round(2)\n",
    "    counts = counts.sort_values([\"Dataset\", \"Warehouse\", \"Count\"], ascending=[True, True, False])\n",
    "    counts[\"Cumulative %\"] = counts.groupby([\"Dataset\", \"Warehouse\"])[\"Count\"].transform(lambda x: (x.cumsum() / x.sum() * 100)).round(2)\n",
    "    return counts\n",
    "\n",
    "def plot_pareto(sub: pd.DataFrame, title: str, top_n=None):\n",
    "    sub_sorted = sub.sort_values(\"Count\", ascending=False)\n",
    "    if top_n is not None:\n",
    "        sub_sorted = sub_sorted.head(int(top_n))\n",
    "\n",
    "    labels = sub_sorted[\"Labels\"].tolist()\n",
    "    counts = sub_sorted[\"Count\"].tolist()\n",
    "    cum_pct = sub_sorted[\"Cumulative %\"].tolist()\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 5))  # one chart per figure\n",
    "    ax1 = plt.gca()\n",
    "    ax1.bar(labels, counts)\n",
    "    ax1.set_xlabel(\"Label\")\n",
    "    ax1.set_ylabel(\"Count\")\n",
    "    ax1.set_title(title)\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(range(len(labels)), cum_pct, marker=\"o\")\n",
    "    ax2.set_ylabel(\"Cumulative %\")\n",
    "    ax2.set_ylim(0, 100)\n",
    "\n",
    "    ax1.tick_params(axis=\"x\", rotation=45, labelsize=9)\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eef1c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# ----------------------------\n",
    "# Load & combine datasets\n",
    "# ----------------------------\n",
    "frames = []\n",
    "missing = []\n",
    "for item in INPUT_FILES:\n",
    "    try:\n",
    "        p = resolve_path(item[\"filename\"])\n",
    "        df_i = load_and_prepare(p, SHEET_NAME, dataset_name=item[\"name\"])\n",
    "        frames.append(df_i)\n",
    "        print(f\"Loaded {len(df_i):,} rows from {item['filename']} as dataset '{item['name']}'\")\n",
    "    except Exception as e:\n",
    "        missing.append((item[\"filename\"], str(e)))\n",
    "        print(f\"WARNING: Skipping '{item['filename']}' -> {e}\")\n",
    "\n",
    "if not frames:\n",
    "    raise RuntimeError(\"No datasets loaded. Ensure input files exist.\")\n",
    "\n",
    "df_all = pd.concat(frames, ignore_index=True)\n",
    "print(f\"Before date filter: {len(df_all):,} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8821a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# ----------------------------\n",
    "# Apply date filter\n",
    "# ----------------------------\n",
    "df_filtered = apply_date_filter(df_all, DATE_FILTER_MODE, START_DATE, END_DATE)\n",
    "print(f\"After date filter ({DATE_FILTER_MODE}, {START_DATE} to {END_DATE}): {len(df_filtered):,} rows\")\n",
    "df_filtered.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77883f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# ----------------------------\n",
    "# Bucket performance (filtered)\n",
    "# ----------------------------\n",
    "bucket_perf_by_dataset = compute_bucket_perf(df_filtered)\n",
    "\n",
    "# Combined ALL view\n",
    "df_filtered_all = df_filtered.copy()\n",
    "df_filtered_all[\"Dataset\"] = \"ALL\"\n",
    "bucket_perf_all = compute_bucket_perf(df_filtered_all)\n",
    "\n",
    "bucket_perf_by_dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2910cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "bucket_perf_all.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8212aac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# ----------------------------\n",
    "# Pareto calculations (filtered)\n",
    "# ----------------------------\n",
    "labels_exploded = explode_labels(df_filtered)\n",
    "\n",
    "# Overall across ALL datasets\n",
    "overall_all = compute_overall_pareto(labels_exploded)\n",
    "\n",
    "# Overall by dataset\n",
    "overall_by_dataset = (\n",
    "    labels_exploded.groupby([\"Dataset\", \"Labels\"])\n",
    "                   .size().reset_index(name=\"Count\")\n",
    "                   .sort_values([\"Dataset\", \"Count\"], ascending=[True, False])\n",
    ")\n",
    "overall_by_dataset[\"Cumulative %\"] = (\n",
    "    overall_by_dataset.groupby(\"Dataset\")[\"Count\"].transform(lambda x: x.cumsum() / x.sum() * 100)\n",
    ").round(2)\n",
    "\n",
    "# By warehouse (Dataset+Warehouse)\n",
    "by_wh = compute_by_wh_pareto(labels_exploded, WAREHOUSES)\n",
    "\n",
    "# Combined ALL by warehouse\n",
    "labels_exploded_all = labels_exploded.copy()\n",
    "labels_exploded_all[\"Dataset\"] = \"ALL\"\n",
    "by_wh_all = compute_by_wh_pareto(labels_exploded_all, WAREHOUSES)\n",
    "\n",
    "overall_all.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb22390b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# ----------------------------\n",
    "# Pareto charts\n",
    "# ----------------------------\n",
    "for ds in sorted(by_wh[\"Dataset\"].unique()):\n",
    "    for wh in sorted(by_wh[by_wh[\"Dataset\"] == ds][\"Warehouse\"].unique()):\n",
    "        sub = by_wh[(by_wh[\"Dataset\"] == ds) & (by_wh[\"Warehouse\"] == wh)]\n",
    "        if not sub.empty:\n",
    "            plot_pareto(sub, f\"Pareto of Labels — {ds} — {wh}\", top_n=TOP_N_LABELS)\n",
    "\n",
    "# Combined ALL for each Warehouse\n",
    "for wh in sorted(by_wh_all[\"Warehouse\"].unique()):\n",
    "    sub = by_wh_all[by_wh_all[\"Warehouse\"] == wh]\n",
    "    if not sub.empty:\n",
    "        plot_pareto(sub, f\"Pareto of Labels — ALL — {wh}\", top_n=TOP_N_LABELS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52f3095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# ----------------------------\n",
    "# Export for Power BI (filtered)\n",
    "# ----------------------------\n",
    "# Merge ALL + dataset-level for overall labels\n",
    "overall_all_export = overall_all.copy()  # Dataset='ALL'\n",
    "overall_by_dataset_export = overall_by_dataset.copy()\n",
    "overall_combined = pd.concat([overall_all_export, overall_by_dataset_export], ignore_index=True)\n",
    "\n",
    "# By-warehouse combined table: stack ALL + per dataset\n",
    "by_wh_combined = pd.concat([by_wh, by_wh_all], ignore_index=True)\n",
    "\n",
    "with pd.ExcelWriter(OUTPUT_XLSX, engine=\"openpyxl\") as writer:\n",
    "    bucket_perf_by_dataset.to_excel(writer, sheet_name=\"Bucket_Performance\", index=False)\n",
    "    bucket_perf_all.to_excel(writer, sheet_name=\"Bucket_Performance_ALL\", index=False)\n",
    "    overall_combined.to_excel(writer, sheet_name=\"Pareto_Labels_Overall\", index=False)\n",
    "    by_wh_combined.to_excel(writer, sheet_name=\"Pareto_Labels_By_Warehouse\", index=False)\n",
    "    df_filtered.to_excel(writer, sheet_name=\"Detailed_Data\", index=False)\n",
    "\n",
    "print(f\"Exported (filtered) Power BI workbook -> {OUTPUT_XLSX.resolve()}\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
